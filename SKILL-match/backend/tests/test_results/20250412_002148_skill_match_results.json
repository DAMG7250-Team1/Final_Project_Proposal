{
  "status": "success",
  "timestamp": "2025-04-12T04:21:48.717319",
  "total_matches": 9,
  "input_skills": [
    "Machine Learning",
    "TensorFlow",
    "MLOps",
    "scikit-learn",
    "AWS Sagemaker",
    "ETL",
    "Power BI",
    "Data Visualization",
    "EDA",
    "Athena",
    "Bayesian Analysis",
    "Pandas",
    "Data Science",
    "Jupyter Notebook",
    "Tableau",
    "CI/CD",
    "Deep Learning",
    "Streamlit",
    "AWS QuickSight",
    "Excel",
    "NumPy",
    "FastAPI",
    "VLOOKUP",
    "GCP",
    "Graph Database",
    "PyTorch",
    "SQL",
    "Kubernetes",
    "Neo4j",
    "VBA",
    "NLP",
    "Python",
    "MySQL",
    "Snowflake",
    "JavaScript",
    "Snowpark",
    "JIRA",
    "Docker",
    "Salesforce",
    "Git",
    "Statistical Modeling"
  ],
  "input_skills_count": 41,
  "matches": [
    {
      "job_id": "job_data_scientist_iii_1744414206",
      "similarity_score": 0.53066349,
      "similarity_category": "medium",
      "job_title": "Data Scientist III",
      "company": "RELX",
      "location": "Alpharetta, GA",
      "job_type": "Full-time",
      "work_mode": "Onsite",
      "seniority": "Mid Level",
      "salary": "",
      "experience": "",
      "responsibilities": "Interfacing with modeling, technology, and product personnel to assure timely and accurate implementation and maintenance of all Insurance attributes and models., Assisting in the development of automated solutions to routine validations in order to gain efficiencies in the validation process., Following analytical and programming guidelines, standards, and best practices., Defining thorough test requirements and objectives., Conducting data manipulation, reporting, exploratory analysis, or AI/ML techniques in support of developing/supporting new products, and/or validating existing product performance., Managing tasks independently, completes tasks efficiently, and embraces team collaboration for multiple concurrent projects. Able to present their work effort, results, and conclusions in a manner appropriate to the audience's level of understanding and is willing and able to ask for help/work with others as necessary., Experience with data analytics, modeling development/validation, quantitative reasoning, data science, or similar., Knowledge of and desire to learn analytical tools, platforms, and programming languages such as Python, PySpark, MySQL, AWS, Azure, and PowerBI., Experience manipulating and merging multiple large data sets in a distributed computing environment., Ability to find solutions hidden in large data sets and the ability to work with stakeholders to improve business outcomes., Possess an attention to detail with a desire to follow a task to completion., Team player, constructive, and considerate of others’ inputs with the ability to positively influence others, confident speaker even in difficult situations., Ability to multi-task and efficiently negotiate changing priorities and responsibilities., Basic understanding of modeling techniques and potential impacts of related assumptions and limitations., Working knowledge of AI/ML methods a plus, Health Benefits: Comprehensive, multi-carrier program for medical, dental and vision benefits, Retirement Benefits: 401(k) with match and an Employee Share Purchase Plan, Wellbeing: Wellness platform with incentives, Headspace app subscription, Employee Assistance and Time-off Programs, Short-and-Long Term Disability, Life and Accidental Death Insurance, Critical Illness, and Hospital Indemnity, Family Benefits, including bonding and family care leaves, adoption and surrogacy benefits, Health Savings, Health Care, Dependent Care and Commuter Spending Accounts, Up to two days of paid leave each to participate in Employee Resource Groups and to volunteer with your charity of choice",
      "qualifications": "Experience with data analytics, modeling development/validation, quantitative reasoning, data science, or similar., Knowledge of and desire to learn analytical tools, platforms, and programming languages such as Python, PySpark, MySQL, AWS, Azure, and PowerBI., Experience manipulating and merging multiple large data sets in a distributed computing environment., Ability to find solutions hidden in large data sets and the ability to work with stakeholders to improve business outcomes., Possess an attention to detail with a desire to follow a task to completion., Team player, constructive, and considerate of others’ inputs with the ability to positively influence others, confident speaker even in difficult situations., Ability to multi-task and efficiently negotiate changing priorities and responsibilities., Basic understanding of modeling techniques and potential impacts of related assumptions and limitations., Working knowledge of AI/ML methods a plus",
      "skills": [
        "Python",
        "PySpark",
        "MySQL",
        "AWS",
        "Azure",
        "PowerBI",
        "Data analytics",
        "Modeling development/validation",
        "AI/ML methods",
        "Quantitative reasoning",
        "Basic modeling techniques"
      ],
      "extracted_skills": [],
      "matching_skills": [],
      "skill_overlap_percent": 0
    },
    {
      "job_id": "job_data_engineer_i_-_marketing_analytics_(hybrid)_1744414213",
      "similarity_score": 0.528931558,
      "similarity_category": "medium",
      "job_title": "Data Engineer I - Marketing Analytics (Hybrid)",
      "company": "Unum",
      "location": "Chattanooga Home Office",
      "job_type": "Full-time",
      "work_mode": "Hybrid",
      "seniority": "Entry Level",
      "salary": "$60K/yr - $123K/yr",
      "experience": "2+ years exp",
      "responsibilities": "Employ a variety of languages and tools to develop, construct, test and maintain data pipelines, ensuring they support the requirements of the business., Integrate varying sizes of data from different sources (including DB2, SQL Server, Web API and Teradata)., Apply validation, aggregation, and reconciliation techniques to create a rich data framework., Work closely with the data scientists and business partners to understand the business problem they are trying to solve and the analytics solutions they plan to apply. Use this understanding to create appropriate data structures tailored for the specific problem., Create data assets that conform to scalability, extensibility, performance and maintainability requirements for the problem at hand. Promotes development of solutions following appropriate engineering process that is fit to purpose for different use scenarios., Understand and contribute to the evolution of the enterprise data architecture including the application of current and emerging data frameworks and tools (eg hosting data in Cloud)., Efficiently prepares results for interpretation and/or visualization and communicates findings and potential value to manager., Support integration of solutions within existing business processes using automation techniques., Understand theory and application of current and emerging software engineering practices., Provides support to lower level Data Engineer peers., Perform other related duties as assigned., Bachelor’s degree in quantitative field, Knowledge in all of the following skillsets and deep expertise in at least two: Software Engineering, Data Architecture and Infrastructure, Holistic Data Preparation, Data Extraction, Transform & Load, Demonstrated communication skills, Experience in financial services, Exposure to working with senior management and executive leadership, Attention to detail while effectively prioritizing work and managing multiple projects simultaneously, Ability to understand and explain a problem and identify and communicate an appropriate solution, Developing ability to coach or mentor team members, Ability to commit quickly and positively to change, Viewed as embracing change and leading proof of concept work and prototyping when necessary, Master’s degree in a quantitative field, 2 years of professional experience or equivalent relevant work experience, Experience in writing complex SQL queries that join multiple tables/databases, Experience exploring databases/tables or other legacy data content to identify best data sources to solve business problems, Demonstrates ability to troubleshoot complex SQL queries with little guidance, Demonstrates ability to create logical data models by combining data from multiple sources including internal and external data, Entrepreneurial self-starter, A thorough, results-oriented problem-solver, A lifelong learner with voracious curiosity, Basic understanding of their organization, Health, Vision, Dental, Short & Long-Term Disability, Generous PTO (including paid time to volunteer!), Up to 9.5% 401(k) employer contribution, Mental health support, Career advancement opportunities, Student loan repayment options, Tuition reimbursement, Flexible work environments",
      "qualifications": "Bachelor’s degree in quantitative field, Knowledge in all of the following skillsets and deep expertise in at least two: Software Engineering, Data Architecture and Infrastructure, Holistic Data Preparation, Data Extraction, Transform & Load, Demonstrated communication skills, Experience in financial services, Exposure to working with senior management and executive leadership, Attention to detail while effectively prioritizing work and managing multiple projects simultaneously, Ability to understand and explain a problem and identify and communicate an appropriate solution, Developing ability to coach or mentor team members, Ability to commit quickly and positively to change, Viewed as embracing change and leading proof of concept work and prototyping when necessary, Master’s degree in a quantitative field, 2 years of professional experience or equivalent relevant work experience, Experience in writing complex SQL queries that join multiple tables/databases, Experience exploring databases/tables or other legacy data content to identify best data sources to solve business problems, Demonstrates ability to troubleshoot complex SQL queries with little guidance, Demonstrates ability to create logical data models by combining data from multiple sources including internal and external data, Entrepreneurial self-starter, A thorough, results-oriented problem-solver, A lifelong learner with voracious curiosity, Basic understanding of their organization",
      "skills": [
        "Data Engineering",
        "SQL",
        "Python",
        "Java",
        "Scala",
        "DevOps",
        "Data Architecture",
        "Data Visualization",
        "DB2",
        "SQL Server",
        "Web API",
        "Teradata",
        "Mentoring"
      ],
      "extracted_skills": [],
      "matching_skills": [],
      "skill_overlap_percent": 0
    },
    {
      "job_id": "job_senior_data_scientist_i_1744414214",
      "similarity_score": 0.505898297,
      "similarity_category": "medium",
      "job_title": "Senior Data Scientist I",
      "company": "RELX",
      "location": "Raleigh, NC",
      "job_type": "Full-time",
      "work_mode": "Remote",
      "seniority": "Senior Level",
      "salary": "",
      "experience": "",
      "responsibilities": "Working closely with other data scientists and engineers to design, develop, and deploy AI solutions., Leading the development of advanced AI and machine learning models to solve complex business problems., Analyzing large datasets to identify patterns and trends that can inform AI development., Collaborating with cross-functional teams to ensure AI solutions are aligned with business goals and customer needs., Evaluating and creating new frameworks and defining methodologies and governance., Mentoring junior data scientists and provide guidance on AI and machine learning best practices., Working with product leaders to apply data science solutions., Leading small- to medium-sized teams (direct or indirect)., Possess an advanced or Master's education in Computer Science, Mathematics, Statistics, or a related field., Have experience working directly with large language models and transformer-based architectures including BERT, RoBERTa, T5 etc., Be proficient in working with big data technologies and tools like Hadoop, Spark, or AWS., Have experience and knowledge of applying LLMs including ChatGPT, GPT 3.5, Claude, Mistral, etc., Be a natural problem solver, able to lead and mentor junior data scientists, and collaborate with cross-functional teams., Demonstrate proficient experience with machine learning algorithms, including deep learning, gradient boosting, and random forests., Possess advanced programming skills in Python, R, or other relevant languages for data analysis., Health Benefits: Comprehensive, multi-carrier program for medical, dental and vision benefits, Retirement Benefits: 401(k) with match and an Employee Share Purchase Plan, Wellbeing: Wellness platform with incentives, Headspace app subscription, Employee Assistance and Time-off Programs, Short-and-Long Term Disability, Life and Accidental Death Insurance, Critical Illness, and Hospital Indemnity, Family Benefits, including bonding and family care leaves, adoption, and surrogacy benefits, Health Savings, Health Care, Dependent Care and Commuter Spending Accounts, Up to two days of paid leave each to participate in Employee Resource Groups and to volunteer with your charity of choice",
      "qualifications": "Possess an advanced or Master's education in Computer Science, Mathematics, Statistics, or a related field., Have experience working directly with large language models and transformer-based architectures including BERT, RoBERTa, T5 etc., Be proficient in working with big data technologies and tools like Hadoop, Spark, or AWS., Have experience and knowledge of applying LLMs including ChatGPT, GPT 3.5, Claude, Mistral, etc., Be a natural problem solver, able to lead and mentor junior data scientists, and collaborate with cross-functional teams., Demonstrate proficient experience with machine learning algorithms, including deep learning, gradient boosting, and random forests., Possess advanced programming skills in Python, R, or other relevant languages for data analysis.",
      "skills": [
        "Large Language Models",
        "Machine Learning Algorithms",
        "Python",
        "Big Data Technologies",
        "Deep Learning",
        "Statistical Analysis",
        "R",
        "Hadoop",
        "Spark",
        "AWS",
        "BERT",
        "RoBERTa",
        "T5",
        "ChatGPT",
        "GPT 3.5",
        "Claude",
        "Mistral",
        "Mentoring"
      ],
      "extracted_skills": [],
      "matching_skills": [],
      "skill_overlap_percent": 0
    },
    {
      "job_id": "job_senior_data_scientist_iii_-_0125-us-011_1744414216",
      "similarity_score": 0.498859584,
      "similarity_category": "low",
      "job_title": "Senior Data Scientist III - 0125-US-011",
      "company": "RELX",
      "location": "Pennsylvania, United States",
      "job_type": "Full-time",
      "work_mode": "Onsite",
      "seniority": "Senior Level",
      "salary": "",
      "experience": "",
      "responsibilities": "Designing and conducting experiments to assess generative AI model performance, Developing metrics, benchmarks, and evaluation frameworks, Creating production-ready Python packages for evaluation pipeline components, Evaluating AI models against healthcare domain standards, Integrating data science components and conduct quality assessments, Developing automated evaluation and maintenance tools, Mentoring junior team members and providing technical leadership, Collaborating with technology teams for pipeline deployment, Implement and maintain product performance reporting, Contributing to new methodologies and governance frameworks, Advanced degree in computer science, data science, AI, mathematics, or statistics, 2+ years post-PhD or 5+ years post-Master's or 10+ years post-Bachelor's relevant experience, Expertise in Generative AI model evaluation and metric development, Proficiency in Python, SQL, R, and Java, Experience with ML algorithms, deep learning, and neural networks, Strong background in GitLab/GitHub and cloud computing, Proven experience with ix systems and Jupyter notebook hubs, Ability to build and test new processes while mentoring others, Track record of developing novel code solutions and best practices, Healthcare domain knowledge highly desirable, Comprehensive, multi-carrier program for medical, dental and vision benefits, 401(k) with match and an Employee Share Purchase Plan, Wellness platform with incentives, Headspace app subscription, Employee Assistance and Time-off Programs, Short-and-Long Term Disability, Life and Accidental Death Insurance, Critical Illness, and Hospital Indemnity, Family Benefits, including bonding and family care leaves, adoption and surrogacy benefits, Health Savings, Health Care, Dependent Care and Commuter Spending Accounts, Up to two days of paid leave each to participate in Employee Resource Groups and to volunteer with your charity of choice",
      "qualifications": "Advanced degree in computer science, data science, AI, mathematics, or statistics, 2+ years post-PhD or 5+ years post-Master's or 10+ years post-Bachelor's relevant experience, Expertise in Generative AI model evaluation and metric development, Proficiency in Python, SQL, R, and Java, Experience with ML algorithms, deep learning, and neural networks, Strong background in GitLab/GitHub and cloud computing, Proven experience with ix systems and Jupyter notebook hubs, Ability to build and test new processes while mentoring others, Track record of developing novel code solutions and best practices, Healthcare domain knowledge highly desirable",
      "skills": [
        "Python",
        "SQL",
        "R",
        "Java",
        "ML algorithms",
        "Deep learning",
        "Neural networks",
        "GitLab/GitHub",
        "Cloud computing",
        "Unix systems",
        "Jupyter notebook",
        "Generative AI evaluation",
        "NLP solutions",
        "Healthcare domain knowledge",
        "Novel code solutions",
        "Mentoring",
        "Technical leadership"
      ],
      "extracted_skills": [],
      "matching_skills": [],
      "skill_overlap_percent": 0
    },
    {
      "job_id": "job_data_analyst_1744414200",
      "similarity_score": 0.48391524,
      "similarity_category": "low",
      "job_title": "Data Analyst",
      "company": "S&P Global",
      "location": "Monterey, CA",
      "job_type": "Full-time",
      "work_mode": "Onsite",
      "seniority": "Mid Level",
      "salary": "$45K/yr - $55K/yr",
      "experience": "",
      "responsibilities": "Interpret and convert data so that it can be utilized by the multiple Market Scan tools and products, Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality, Acquire data from primary or secondary data sources and maintain databases/data systems, Identify, analyze, and interpret trends or patterns in complex data sets, Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct problems, Work with management to prioritize business and information needs, Locate and define new process improvement opportunities, Edit it using various software tools, to include Adobe Acrobat DC and Excel’s Power Query, Proven working experience as a data analyst or business data analyst, Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy, Adept at queries, report writing and presenting finding, Strong computer aptitude with intermediate Microsoft Office experience with products like Excel, VBA, Word, Outlook, and SQL; web scraping is a plus., Ability to handle extended periods of computer-based work, Communication: Must have superior written and oral communication skills. Tailors the content of their communications to the level and experience of the audience., Results Orientation: Prioritizes projects to meet required deadlines; ability to manage several projects at once focusing on the desired end result of one’s own and business unit’s work., Stress Management: Is a master multi-tasker and problem solver who can prioritize matters while delivering high-level results. Remains calm under pressure., Health & Wellness: Health care coverage designed for the mind and body., Flexible Downtime: Generous time off helps keep you energized for your time on., Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills., Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs., Family Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families., Beyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.",
      "qualifications": "Proven working experience as a data analyst or business data analyst, Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy, Adept at queries, report writing and presenting finding, Strong computer aptitude with intermediate Microsoft Office experience with products like Excel, VBA, Word, Outlook, and SQL; web scraping is a plus., Ability to handle extended periods of computer-based work, Communication: Must have superior written and oral communication skills. Tailors the content of their communications to the level and experience of the audience., Results Orientation: Prioritizes projects to meet required deadlines; ability to manage several projects at once focusing on the desired end result of one’s own and business unit’s work., Stress Management: Is a master multi-tasker and problem solver who can prioritize matters while delivering high-level results. Remains calm under pressure.",
      "skills": [
        "Data Analysis",
        "Database Development",
        "Statistical Efficiency",
        "Microsoft Excel",
        "SQL",
        "VBA",
        "Data Cleaning",
        "Web Scraping",
        "Adobe Acrobat DC",
        "Microsoft Word",
        "Microsoft Outlook",
        "Report Writing"
      ],
      "extracted_skills": [],
      "matching_skills": [],
      "skill_overlap_percent": 0
    },
    {
      "job_id": "job_software_engineer_ii_1744414203",
      "similarity_score": 0.46177271,
      "similarity_category": "low",
      "job_title": "Software Engineer II",
      "company": "Tempus AI",
      "location": "Chicago, IL",
      "job_type": "Full-time",
      "work_mode": "Onsite",
      "seniority": "Mid Level",
      "salary": "$85K/yr - $130K/yr",
      "experience": "",
      "responsibilities": "You've got software development experience in both frontend and backend technologies., You're familiar with some of the technologies we use, such as: Node.js, React.js, Angular.js, Java, Python, MySQL, Postgres, REST, and GraphQL., You've worked in agile environments and are comfortable iterating quickly., You enjoy collaborating and communicating with team members of varying skill sets and from different fields., Software development experience in both frontend and backend technologies., Familiarity with some of the technologies we use, such as: Node.js, React.js, Angular.js, Java, Python, MySQL, Postgres, REST, and GraphQL., Experience working in agile environments and comfortable iterating quickly., Enjoy collaborating and communicating with team members of varying skill sets and from different fields., Experience in Docker, Experience in AWS, GCP, Experience in Spark, Hadoop, Kafka, etc., An interest in working in biotech, genomics, and precision medicine, Experience with some of the technologies we use such as Kubernetes, BigQuery, or GCP’s Machine Learning infrastructure., Experience working in fast-paced, agile environments., Incentive compensation, Restricted stock units, Medical and other benefits",
      "qualifications": "Software development experience in both frontend and backend technologies., Familiarity with some of the technologies we use, such as: Node.js, React.js, Angular.js, Java, Python, MySQL, Postgres, REST, and GraphQL., Experience working in agile environments and comfortable iterating quickly., Enjoy collaborating and communicating with team members of varying skill sets and from different fields., Experience in Docker, Experience in AWS, GCP, Experience in Spark, Hadoop, Kafka, etc., An interest in working in biotech, genomics, and precision medicine, Experience with some of the technologies we use such as Kubernetes, BigQuery, or GCP’s Machine Learning infrastructure., Experience working in fast-paced, agile environments.",
      "skills": [
        "Node.js",
        "React.js",
        "Python",
        "Java",
        "MySQL",
        "Postgres",
        "REST",
        "GraphQL",
        "Angular.js",
        "Docker",
        "AWS",
        "GCP",
        "Spark",
        "Hadoop",
        "Kafka",
        "Kubernetes",
        "BigQuery",
        "Machine Learning",
        "Biotech",
        "Genomics",
        "Precision Medicine",
        "Agile Environments"
      ],
      "extracted_skills": [],
      "matching_skills": [],
      "skill_overlap_percent": 0
    },
    {
      "job_id": "job_ai_engineer,_multimodal_models_1744414202",
      "similarity_score": 0.440644473,
      "similarity_category": "low",
      "job_title": "AI Engineer, Multimodal Models",
      "company": "CliniComp",
      "location": "San Diego, CA",
      "job_type": "Full-time",
      "work_mode": "Hybrid",
      "seniority": "Mid Level",
      "salary": "",
      "experience": "",
      "responsibilities": "Contribute to innovative projects in the field of the multimodal foundation models for LLM, image, and clinical data analysis., Work with a dynamic team who is pushing the boundaries of AI machine learning in the healthcare domain and building real-world products for electronic health record (EHR) system., Research best practices for data curation, preparation and training of our state-of-the-art foundation models., Address some of the hardest problems in computer science., Location: Please be aware that the job position requires the candidate to be located within the San Diego metropolitan area, Master's degree in Computer Science with a focus on LLM and image multimodal foundation models, Excellent programming skills in Python and C/C++, Proficiency in PyTorch framework and the common NLP libraries (NLTK, spaCy, scikit-learn, etc.), Experience with state-of-the-art deep learning architectures (ViT/SWIN transformer, GPT, CLIP, etc.), Experience with model compression and large-scale distributed model training (data-parallel and model-parallel) techniques, An outstanding track record of publications (NeurIPS, CVPR, ICML, AAAI, etc.) and contributions to the machine learning communities (kaggle, Hugging Face, etc.), Hands-on experience with parameter-efficient tuning (QLoRA) techniques and the LangChain framework, Experience with prompt engineering and fine-tuning Llama 2 or PaLM 2 with domain specific data, Experience with CUDA programming, Experience with imaging processing (opencv2, VTK, ITK, DCMTK, Albumentations), Experience with vector databases (Chroma, Pinecone, Milvus, redis, etc.), Experience with human-in-the-loop, Reinforcement Learning from Human Feedback (RLHF), and continuous online training, Knowledge for Stable Diffusion, OpenJourney, or DeepFloyd IF, 100% covered Medical and Dental coverage option for you and your family, Generous 401(k) plan and contribution, Events and biweekly lunches, Engaging wellness activities including an onsite nutritionist and personal trainer-led group fitness",
      "qualifications": "Location: Please be aware that the job position requires the candidate to be located within the San Diego metropolitan area, Master's degree in Computer Science with a focus on LLM and image multimodal foundation models, Excellent programming skills in Python and C/C++, Proficiency in PyTorch framework and the common NLP libraries (NLTK, spaCy, scikit-learn, etc.), Experience with state-of-the-art deep learning architectures (ViT/SWIN transformer, GPT, CLIP, etc.), Experience with model compression and large-scale distributed model training (data-parallel and model-parallel) techniques, An outstanding track record of publications (NeurIPS, CVPR, ICML, AAAI, etc.) and contributions to the machine learning communities (kaggle, Hugging Face, etc.), Hands-on experience with parameter-efficient tuning (QLoRA) techniques and the LangChain framework, Experience with prompt engineering and fine-tuning Llama 2 or PaLM 2 with domain specific data, Experience with CUDA programming, Experience with imaging processing (opencv2, VTK, ITK, DCMTK, Albumentations), Experience with vector databases (Chroma, Pinecone, Milvus, redis, etc.), Experience with human-in-the-loop, Reinforcement Learning from Human Feedback (RLHF), and continuous online training, Knowledge for Stable Diffusion, OpenJourney, or DeepFloyd IF",
      "skills": [
        "Python",
        "C/C++",
        "PyTorch",
        "Deep learning architectures",
        "NLP libraries",
        "Model compression",
        "Distributed model training",
        "Publications in ML",
        "Parameter-efficient tuning",
        "Prompt engineering",
        "CUDA programming",
        "Image processing",
        "Vector databases",
        "Human-in-the-loop",
        "Reinforcement Learning from Human Feedback",
        "Stable Diffusion"
      ],
      "extracted_skills": [],
      "matching_skills": [],
      "skill_overlap_percent": 0
    },
    {
      "job_id": "job_senior_software_engineer_i_1744414211",
      "similarity_score": 0.439096957,
      "similarity_category": "low",
      "job_title": "Senior Software Engineer I",
      "company": "RELX",
      "location": "Florida, United States",
      "job_type": "Full-time",
      "work_mode": "Onsite",
      "seniority": "Senior Level",
      "salary": "",
      "experience": "",
      "responsibilities": "Designing and developing scalable and high-performance applications using NodeJS for backend and Angular for front-end development., Architecting solutions with a focus on data analytics, visualizations, and real-time performance metrics that aid in crime prevention and resolution., Collaborating with cross-functional teams to ensure seamless integration of frontend and backend services, ensuring a fluid user experience for public safety officials and community users., Optimizing applications for maximum speed, scalability, and security, ensuring compliance with best practices and regulatory standards., Developing reusable components and libraries that can be leveraged across multiple applications., Leading efforts in code reviews, testing, and continuous integration to ensure robust, reliable, and maintainable codebases., Current experience with NodeJS and Angular in building scalable and high-performance applications., Understanding of modern front-end and back-end frameworks (NextJS) and how they contribute to a seamless product experience., Experience with data analytics, visualization frameworks, and tools that can process and display large sets of data in real-time (e.g., D3.js, Chart.js)., Knowledge of performance optimization techniques for both client-side and server-side applications., Experience building RESTful APIs and working with databases like MySQL, MongoDB, or similar., Experience with building Docker images and Kubernetes deployments, services, ingress, config maps and secrets., Proficiency in implementing secure and compliant coding practices, especially within public safety or sensitive data contexts., Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization tools like Docker or Kubernetes., Familiarity with microservices architecture and best practices for building distributed systems., Knowledge of AI or machine learning techniques that can assist with predictive policing or incident trend analysis., Health Benefits: Comprehensive, multi-carrier program for medical, dental and vision benefits, Retirement Benefits: 401(k) with match and an Employee Share Purchase Plan, Wellbeing: Wellness platform with incentives, Headspace app subscription, Employee Assistance and Time-off Programs, Short-and-Long Term Disability, Life and Accidental Death Insurance, Critical Illness, and Hospital Indemnity, Family Benefits, including bonding and family care leaves, adoption and surrogacy benefits, Health Savings, Health Care, Dependent Care and Commuter Spending Accounts, In addition to annual Paid Time Off, we offer up to two days of paid leave each to participate in Employee Resource Groups and to volunteer with your charity of choice",
      "qualifications": "Current experience with NodeJS and Angular in building scalable and high-performance applications., Understanding of modern front-end and back-end frameworks (NextJS) and how they contribute to a seamless product experience., Experience with data analytics, visualization frameworks, and tools that can process and display large sets of data in real-time (e.g., D3.js, Chart.js)., Knowledge of performance optimization techniques for both client-side and server-side applications., Experience building RESTful APIs and working with databases like MySQL, MongoDB, or similar., Experience with building Docker images and Kubernetes deployments, services, ingress, config maps and secrets., Proficiency in implementing secure and compliant coding practices, especially within public safety or sensitive data contexts., Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization tools like Docker or Kubernetes., Familiarity with microservices architecture and best practices for building distributed systems., Knowledge of AI or machine learning techniques that can assist with predictive policing or incident trend analysis.",
      "skills": [
        "NodeJS",
        "Angular",
        "RESTful APIs",
        "NextJS",
        "D3.js",
        "Chart.js",
        "MySQL",
        "MongoDB",
        "Docker",
        "Kubernetes",
        "AWS",
        "Azure",
        "GCP",
        "Microservices architecture",
        "AI techniques",
        "Machine learning techniques",
        "Performance optimization",
        "Secure coding practices"
      ],
      "extracted_skills": [],
      "matching_skills": [],
      "skill_overlap_percent": 0
    },
    {
      "job_id": "job_web_developer_1744414217",
      "similarity_score": 0.399436623,
      "similarity_category": "low",
      "job_title": "Web Developer",
      "company": "Airtable",
      "location": "San Francisco, CA",
      "job_type": "Full-time",
      "work_mode": "Onsite",
      "seniority": "Mid Level",
      "salary": "$145K/yr - $175K/yr",
      "experience": "3+ years exp",
      "responsibilities": "Build easy-to-use, performant websites, delivering seamless user experiences for multiple partners across marketing and design., Take ownership of both front-end and back-end services, including implementation, optimization, debugging, and overall reliability. Propose solutions for improving usability, performance, and scalability., Work across the full stack, using technologies such as React, Next.js, Node.js, and modern CSS tools., Participate in the full development lifecycle, including estimation, design, development, testing, and deployment, with a focus on delivering end-to-end solutions., Serve as a lead and mentor to the web engineering team, guiding them in both front-end and back-end development practices., Identify and resolve issues across the stack, ensuring scalability, reliability, and high performance in all aspects of the application., Engineer with 3+ years of experience, Willing to learn new technologies and apply them in fast-paced environment, Familiar with Node.JS, Next.JS is a plus, Experience with standard web technology, including HTML, CSS, JavaScript, TypeScript, and React, Passionate about software engineering best practices, including agile development, unit testing, code reviews, design documentation, debugging, and troubleshooting., Demonstrated critical thinking, decision-making, and problem-solving skills, Can balance many simultaneous projects, and thrive in fast-paced environments, Experience in Heroku and github actions, Experience with headless CMS, Familiar with CI/CD, Benefits, Restricted stock units, Incentive compensation",
      "qualifications": "Engineer with 3+ years of experience, Willing to learn new technologies and apply them in fast-paced environment, Familiar with Node.JS, Next.JS is a plus, Experience with standard web technology, including HTML, CSS, JavaScript, TypeScript, and React, Passionate about software engineering best practices, including agile development, unit testing, code reviews, design documentation, debugging, and troubleshooting., Demonstrated critical thinking, decision-making, and problem-solving skills, Can balance many simultaneous projects, and thrive in fast-paced environments, Experience in Heroku and github actions, Experience with headless CMS, Familiar with CI/CD",
      "skills": [
        "React",
        "Node.js",
        "Next.js",
        "JavaScript",
        "TypeScript",
        "HTML",
        "CSS",
        "Heroku",
        "GitHub Actions",
        "Headless CMS",
        "CI/CD",
        "Agile Development",
        "Unit Testing",
        "Code Reviews",
        "Design Documentation",
        "Debugging"
      ],
      "extracted_skills": [],
      "matching_skills": [],
      "skill_overlap_percent": 0
    }
  ]
}